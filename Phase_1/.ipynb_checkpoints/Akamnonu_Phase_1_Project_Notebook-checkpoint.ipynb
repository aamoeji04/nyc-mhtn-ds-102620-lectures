{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries\n",
    "First, we need to import the necessary libraries and packages to create this data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the Data\n",
    "After establishing the libraries, we can gather the location of our data on the Box Office Mojo webpage. \n",
    "We will abreviate Box Office Mojo as 'BOM' from now on with regard to code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty list for which our data can be stored\n",
    "BOM_data = []\n",
    "# create a list of the web page numbers you would like read\n",
    "pages = [200, 400, 600, 800]\n",
    "# create a loop that will iterate thru every page in our range to reterive the data\n",
    "for pg in pages:\n",
    "#     request the content of the webpage\n",
    "    bom_response = requests.get('https://www.boxofficemojo.com/chart/ww_top_lifetime_gross/?area=XWW&offset='+str(pg))\n",
    "#     parse thru the html content using the Beautfiul Soup package\n",
    "    bom_html = BeautifulSoup(bom_response.content, 'html.parser')\n",
    "#     select all movie data containers to iterate thru\n",
    "    bom_containers = bom_html.find('table').findAll('tr')\n",
    "#     for each movie of each webpage in the containers list, gather the following information\n",
    "    for x in bom_containers:\n",
    "        rank = x.findAll('td')[1].get_text()\n",
    "        name = x.findAll('td')[2].get_text()\n",
    "        year = x.findAll('td')[-1].get_text()\n",
    "        ww_gross = x.findAll('td')[3].get_text()\n",
    "        dom_gross = x.findAll('td')[4].get_text()\n",
    "        dom_perc = x.findAll('td')[5].get_text()\n",
    "        fore_gross = x.findAll('td')[6].get_text()\n",
    "        fore_perc = x.findAll('td')[7].get_text()\n",
    "#     compile all of the aquired data into a list and assign to a variable for easy access\n",
    "        data = [rank, name, year, ww_gross, dom_gross, dom_perc, fore_gross, fore_perc]\n",
    "#     append the listed data into the empty list we created earlier\n",
    "        BOM_data.append(data)\n",
    "BOM_data\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
